{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, lead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARK SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"presentation\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load 2) Temporary View 3) Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame from the CSV file with headers\n",
    "df = spark.read.csv(\"train.csv\", header=True)\n",
    "\n",
    "# Register the DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"train_view\")\n",
    "\n",
    "# Run an SQL query on the temporary view\n",
    "result = spark.sql(\"SELECT * FROM train_view\")  # Replace with your desired SQL query\n",
    "result.show()\n",
    "\n",
    "# Inspect the table schema\n",
    "result=spark.sql(\"SHOW COLUMN FROM train_view\")\n",
    "result=spark.sql(\"SELECT * FROM train_view LIMIT 0\")\n",
    "result=spark.sql(\"DESCRIBE train_view\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Rename a column using SQL query and DF dot Notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"train_view\")\n",
    "df = spark.sql(\"SELECT id AS train_id, station, time FROM train_view\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.withColumnRenamed(\"id\", \"train_id\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.select(col(\"id\").alias(\"train_id\"), \"station\", \"time\")\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Adding an ID using Window Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a SQL query to add a unique ID using ROW_NUMBER()\n",
    "sql_query = \"\"\"\n",
    "SELECT train_id, station, time, ROW_NUMBER() OVER (PARTITION BY train_id ORDER BY time) AS id\n",
    "FROM train_view\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query using Spark SQL\n",
    "df4 = spark.sql(sql_query)\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a window specification\n",
    "window_spec = Window.partitionBy(\"train_id\").orderBy(\"time\")\n",
    "\n",
    "# Add a unique ID column using DataFrame dot notation\n",
    "df5 = df.withColumn(\"id\", row_number().over(window_spec))\n",
    "\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Adding a new column with time until next stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column \"time_next\" using SQL query\n",
    "sql_query = \"\"\"\n",
    "SELECT id, station, time,\n",
    "       LEAD(time, 1) OVER (PARTITION BY id ORDER BY time) AS time_next\n",
    "FROM train_view\n",
    "\"\"\"\n",
    "\n",
    "df_with_time_next = spark.sql(sql_query)\n",
    "\n",
    "# Calculate the time difference using another SQL query\n",
    "df_with_time_next.createOrReplaceTempView(\"train_view_with_time_next\")\n",
    "\n",
    "sql_query2 = \"\"\"\n",
    "SELECT id, station, time, time_next,\n",
    "       UNIX_TIMESTAMP(time_next, 'HH:mm') - UNIX_TIMESTAMP(time, 'HH:mm') AS time_difference\n",
    "FROM train_view_with_time_next\n",
    "\"\"\"\n",
    "\n",
    "df_with_time_difference = spark.sql(sql_query2)\n",
    "df_with_time_difference.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a window specification to order by time\n",
    "window_spec = Window.partitionBy(\"id\").orderBy(\"time\")\n",
    "\n",
    "# Add a new column \"time_next\" using LEAD function\n",
    "df_with_time_next = df.withColumn(\"time_next\", lead(\"time\", 1).over(window_spec))\n",
    "\n",
    "# Calculate the time difference between the current and next stop\n",
    "df_with_time_difference = df_with_time_next.withColumn(\"time_difference\",\n",
    "                                                       (F.unix_timestamp(\"time_next\", \"HH:mm\") -\n",
    "                                                        F.unix_timestamp(\"time\", \"HH:mm\")).cast(\"int\"))\n",
    "\n",
    "df_with_time_difference.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
